{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"images/sentiment.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In questo tutorial vediamo come creare un modello di sentiment analysis in Keras.\n",
    "\n",
    "Per creare classificatori di testo i passi fondamentali sono:\n",
    "    - Embed\n",
    "    - Encode\n",
    "    - (Attend)\n",
    "    - Predict\n",
    "\n",
    "[Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models](https://explosion.ai/blog/deep-learning-formula-nlp)\n",
    "\n",
    "Vediamo passo passo questi passaggi reimplementando il modello stato dell'arte per il sentiment analysis a Semeval 2015.\n",
    "\n",
    "[Twitter sentiment analysis with deep convolutional neural networks](https://pdfs.semanticscholar.org/9320/a229b450bee8384f218681634e039acd9c2f.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and embeddings can be downloaded from [here](https://drive.google.com/open?id=0B8xjf4y9r8jCdVFjVTZqdzZTbVU)\n",
    "\n",
    "\n",
    "\n",
    "Come prima cosa prepariamo i dati per il training facendo del preprocessing. Questo [tokenizer](https://github.com/jaredks/tweetokenize) applica delle semplici trasformazioni al tweet: \n",
    "\n",
    "- lowercase\n",
    "- mappa i numeri in ad un token speciale NUMBER\n",
    "- mappa il nome utente ad un carattere speciale USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tkn = Tokenizer()\n",
    "\n",
    "def preprocess(tweet):\n",
    "    return tkn.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'USERNAME',\n",
       " u'gas',\n",
       " u'by',\n",
       " u'my',\n",
       " u'house',\n",
       " u'hit',\n",
       " u'NUMBER',\n",
       " u'!',\n",
       " u'!',\n",
       " u'!',\n",
       " u'!',\n",
       " u\"i'm\",\n",
       " u'going',\n",
       " u'to',\n",
       " u'chapel',\n",
       " u'hill',\n",
       " u'on',\n",
       " u'sat',\n",
       " u'.',\n",
       " u':)',\n",
       " u'#lol']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"@bestuser Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :) #lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo preprocessare il training set di Semeval 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'excuse',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'this',\n",
       " u'live',\n",
       " u'stream',\n",
       " u',',\n",
       " u'from',\n",
       " u'baba',\n",
       " u'amr',\n",
       " u',',\n",
       " u'so',\n",
       " u'many',\n",
       " u'activists',\n",
       " u'using',\n",
       " u'only',\n",
       " u'one',\n",
       " u'sat',\n",
       " u'modem',\n",
       " u'.',\n",
       " u'LIVE',\n",
       " u'URL',\n",
       " u'#Homs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(file_name, gold=None):\n",
    "    labels = {'negative':0, 'neutral':1, 'positive':2, 'unknwn':1}\n",
    "    X_, y_ = [], []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            label, _, text = line.strip().split('\\t') \n",
    "            y_.append(labels[label])\n",
    "            X_.append(preprocess(text))\n",
    "    if gold:\n",
    "        y_ = []\n",
    "        with open(gold) as f:\n",
    "            for line in f:\n",
    "                _, _, label = line.strip().split('\\t')\n",
    "                y_.append(labels[label])\n",
    "    return (X_, y_)\n",
    "            \n",
    "X_train, y_train = load_dataset(\"data/train'13.csv\")\n",
    "X_dev, y_dev = load_dataset(\"data/dev'13.csv\")\n",
    "X_train = X_train+X_dev\n",
    "y_train = y_train+y_dev\n",
    "X_dev, y_dev = load_dataset(\"data/test'13.csv\")\n",
    "X_test_1, _ = load_dataset(\"data/sms'13.csv\")\n",
    "X_test_2, _ = load_dataset(\"data/test'14.csv\")\n",
    "X_test_3, _ = load_dataset(\"data/test'15.csv\")\n",
    "X_dev[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per velocizzare il mapping assegnamo creiamo un dizionario dove ad ogni parola è assegnato un Id univoco. In questo dizionario aggiungiamo una token speciale per le parole sconosciute e uno per il PADDING (Spigherò dopo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33609"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "dictionary = {'PAD':0, 'UNK':1}\n",
    "\n",
    "toks = set(chain.from_iterable(X_train+X_dev+X_test_1+X_test_2+X_test_3))\n",
    "for i, tok in enumerate(toks):\n",
    "    dictionary[tok] = i+2\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora mappiamo le parole di training e dev set a questi indici nel dizionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20017,\n",
       " 10819,\n",
       " 13784,\n",
       " 15722,\n",
       " 26775,\n",
       " 11018,\n",
       " 32099,\n",
       " 29495,\n",
       " 14115,\n",
       " 9877,\n",
       " 3011,\n",
       " 29495,\n",
       " 17467,\n",
       " 17670,\n",
       " 1314,\n",
       " 18253,\n",
       " 19701,\n",
       " 733,\n",
       " 25967,\n",
       " 26452,\n",
       " 337,\n",
       " 265,\n",
       " 9510,\n",
       " 29007]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2id(sent):\n",
    "    return map(lambda x: dictionary.get(x, 1), sent)\n",
    "\n",
    "X_train = map(word2id, X_train)\n",
    "X_dev = map(word2id, X_dev)\n",
    "    \n",
    "X_dev[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generale le reti neurali accettano solo vettori (tensori) di dimensione prefissata quindi mapperemo tutte le frasi alla frase piu lunga del training set. E convertiamo le frasi in vettori numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0, 14158, 10819, 32671,\n",
       "       32837, 22163, 10819,  4200, 26828, 27247, 25524, 32837, 18384,\n",
       "       28365,  2483, 17877,  5846,  7431, 22140, 25735, 14638, 28244,\n",
       "       13358, 12938,   105, 16485,   337], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_len = max(len(x) for x in X_train)\n",
    "\n",
    "def _pad(s, maxlen):\n",
    "    pad_ = np.zeros(maxlen, dtype='int32')\n",
    "    trunc = np.asarray(s[-maxlen:], dtype='int32')\n",
    "    pad_[-len(trunc):] = trunc\n",
    "    return pad_\n",
    "\n",
    "X_train = np.array(map(lambda x: _pad(x, max_len), X_train))\n",
    "X_dev = np.array(map(lambda x: _pad(x, max_len), X_dev))\n",
    "X_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come ultimo passaggio mappiamo le labels delle tre classi in one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       ..., \n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labels(x):\n",
    "    out_ = np.zeros(3, dtype='int32')\n",
    "    out_[x] = 1\n",
    "    return out_\n",
    "\n",
    "y_train = np.array(map(labels, y_train))\n",
    "y_dev = np.array(map(labels, y_dev))\n",
    "y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "A questo punto definiamo la network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 68, 100)       3360900     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 68, 300)       150300      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "globalmaxpooling1d_4 (GlobalMaxP (None, 300)           0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 300)           0           globalmaxpooling1d_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 3)             903         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3512103\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Convolution1D,\n",
    "                          GlobalMaxPooling1D,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), emb_dim, input_length=max_len)) #Embed\n",
    "model.add(Convolution1D(nb_filter=conv_filters, filter_length=5, border_mode='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D()) #Encode\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11338 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "11338/11338 [==============================] - 81s - loss: 0.9019 - acc: 0.5403 - val_loss: 0.8527 - val_acc: 0.5746\n",
      "Epoch 2/1000\n",
      "11338/11338 [==============================] - 76s - loss: 0.8135 - acc: 0.6095 - val_loss: 0.7833 - val_acc: 0.6155\n",
      "Epoch 3/1000\n",
      "11338/11338 [==============================] - 72s - loss: 0.7433 - acc: 0.6496 - val_loss: 0.7498 - val_acc: 0.6397\n",
      "Epoch 4/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.6862 - acc: 0.6844 - val_loss: 0.7258 - val_acc: 0.6551\n",
      "Epoch 5/1000\n",
      "11338/11338 [==============================] - 70s - loss: 0.6400 - acc: 0.7115 - val_loss: 0.7265 - val_acc: 0.6549\n",
      "Epoch 6/1000\n",
      "11338/11338 [==============================] - 70s - loss: 0.5947 - acc: 0.7344 - val_loss: 0.6967 - val_acc: 0.6737\n",
      "Epoch 7/1000\n",
      "11338/11338 [==============================] - 70s - loss: 0.5535 - acc: 0.7559 - val_loss: 0.6986 - val_acc: 0.6798\n",
      "Epoch 8/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.5178 - acc: 0.7768 - val_loss: 0.6932 - val_acc: 0.6824\n",
      "Epoch 9/1000\n",
      "11338/11338 [==============================] - 72s - loss: 0.4788 - acc: 0.7978 - val_loss: 0.6990 - val_acc: 0.6835\n",
      "Epoch 10/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.4454 - acc: 0.8146 - val_loss: 0.7028 - val_acc: 0.6824\n",
      "Epoch 11/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.4117 - acc: 0.8300 - val_loss: 0.7236 - val_acc: 0.6782\n",
      "Epoch 12/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.3810 - acc: 0.8450 - val_loss: 0.7334 - val_acc: 0.6785\n",
      "Epoch 13/1000\n",
      "11338/11338 [==============================] - 71s - loss: 0.3498 - acc: 0.8608 - val_loss: 0.7309 - val_acc: 0.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114542b10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          nb_epoch=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n",
      "Semeval F1 score: 55.7038501905 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.59      0.50       394\n",
      "          1       0.82      0.69      0.75      1208\n",
      "          2       0.59      0.64      0.61       492\n",
      "\n",
      "avg / total       0.69      0.66      0.67      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 59.0717948718 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.40      0.48       601\n",
      "          1       0.65      0.82      0.73      1640\n",
      "          2       0.75      0.65      0.70      1572\n",
      "\n",
      "avg / total       0.69      0.68      0.68      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 52.4286331433 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.37      0.42       365\n",
      "          1       0.59      0.81      0.68       987\n",
      "          2       0.74      0.54      0.63      1038\n",
      "\n",
      "avg / total       0.64      0.63      0.62      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load_word2vec_format('data/embeddings.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_matrix(dictionary, model):\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(dictionary), 100))\n",
    "    for word in dictionary:\n",
    "        if word in model:\n",
    "            embedding_matrix[dictionary[word]] = model[word]\n",
    "    return embedding_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 68, 100)       3360900     embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 68, 300)       150300      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "globalmaxpooling1d_2 (GlobalMaxP (None, 300)           0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 300)           0           globalmaxpooling1d_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             903         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3512103\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Convolution1D,\n",
    "                          GlobalMaxPooling1D,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "embeddings = Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embeddings) #Embed\n",
    "model.add(Convolution1D(nb_filter=conv_filters, filter_length=5, border_mode='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D()) #Encode\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11338 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "11338/11338 [==============================] - 84s - loss: 0.6433 - acc: 0.7269 - val_loss: 0.6592 - val_acc: 0.7013\n",
      "Epoch 2/1000\n",
      "11338/11338 [==============================] - 78s - loss: 0.5796 - acc: 0.7556 - val_loss: 0.6585 - val_acc: 0.7039\n",
      "Epoch 3/1000\n",
      "11338/11338 [==============================] - 75s - loss: 0.5328 - acc: 0.7781 - val_loss: 0.6610 - val_acc: 0.7005\n",
      "Epoch 4/1000\n",
      "11338/11338 [==============================] - 91s - loss: 0.4900 - acc: 0.8007 - val_loss: 0.6445 - val_acc: 0.7173\n",
      "Epoch 5/1000\n",
      "11338/11338 [==============================] - 78s - loss: 0.4550 - acc: 0.8198 - val_loss: 0.6861 - val_acc: 0.6968\n",
      "Epoch 6/1000\n",
      "11338/11338 [==============================] - 82s - loss: 0.4194 - acc: 0.8359 - val_loss: 0.6570 - val_acc: 0.7157\n",
      "Epoch 7/1000\n",
      "11338/11338 [==============================] - 85s - loss: 0.3843 - acc: 0.8497 - val_loss: 0.6612 - val_acc: 0.7189\n",
      "Epoch 8/1000\n",
      "11338/11338 [==============================] - 74s - loss: 0.3470 - acc: 0.8720 - val_loss: 0.6825 - val_acc: 0.7152\n",
      "Epoch 9/1000\n",
      "11338/11338 [==============================] - 78s - loss: 0.3211 - acc: 0.8845 - val_loss: 0.6947 - val_acc: 0.7118\n",
      "Epoch 10/1000\n",
      "11338/11338 [==============================] - 74s - loss: 0.2856 - acc: 0.8993 - val_loss: 0.6841 - val_acc: 0.7173\n",
      "Epoch 11/1000\n",
      "11338/11338 [==============================] - 75s - loss: 0.2563 - acc: 0.9145 - val_loss: 0.6917 - val_acc: 0.7144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x106b5afd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          nb_epoch=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n",
      "Semeval F1 score: 65.3348018351 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.81      0.64       394\n",
      "          1       0.87      0.73      0.79      1208\n",
      "          2       0.68      0.66      0.67       492\n",
      "\n",
      "avg / total       0.76      0.73      0.73      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 66.6498282735 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.58      0.62       601\n",
      "          1       0.68      0.84      0.75      1640\n",
      "          2       0.81      0.64      0.72      1572\n",
      "\n",
      "avg / total       0.73      0.72      0.72      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 60.8250776346 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.60      0.55       365\n",
      "          1       0.62      0.79      0.70       987\n",
      "          2       0.83      0.56      0.67      1038\n",
      "\n",
      "avg / total       0.70      0.66      0.66      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 68, 100)       3360900     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 68, 300)       225900      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "gru_2 (GRU)                      (None, 300)           540900      bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 300)           0           gru_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 3)             903         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 4128603\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          GRU,\n",
    "                          Bidirectional,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], dropout=0.2)) #Embed\n",
    "model.add(Bidirectional(GRU(150, activation='relu', return_sequences=True)))\n",
    "model.add(GRU(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy', 'fbeta_score'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          nb_epoch=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiniamo un convolutional model con un Recurrent model. Prima creaiamo le rappresentazioni dei 5-gram usando la convoluzione. Per velocizzare il training riduciamo la lunghezza dell'input usando Max pooling (4). Questo layer esegue l'operazione di max pooling non a livello di frase ma ogni 4 n-gram embeddings. In seguito usiamo gated recurrent unit per ottenere il vettore in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 68, 100)       3360900     embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_3 (Convolution1D)  (None, 68, 300)       150300      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 17, 300)       0           convolution1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "gru_3 (GRU)                      (None, 300)           540900      maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 3)             903         gru_3[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 4053003\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Convolution1D,\n",
    "                          MaxPooling1D,\n",
    "                          GRU,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "embeddings = Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embeddings) #Embed\n",
    "model.add(Convolution1D(nb_filter=conv_filters, filter_length=5, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(4)) #Encode\n",
    "model.add(GRU(300, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy', 'fbeta_score'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          nb_epoch=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
