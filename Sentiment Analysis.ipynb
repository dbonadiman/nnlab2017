{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"images/sentiment.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In questo tutorial vediamo come creare un modello di sentiment analysis in Keras.\n",
    "\n",
    "Per creare classificatori di testo i passi fondamentali sono:\n",
    "    - Embed\n",
    "    - Encode\n",
    "    - (Attend)\n",
    "    - Predict\n",
    "\n",
    "[Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models](https://explosion.ai/blog/deep-learning-formula-nlp)\n",
    "\n",
    "Vediamo passo passo questi passaggi reimplementando il modello stato dell'arte per il sentiment analysis a Semeval 2015.\n",
    "\n",
    "[Twitter sentiment analysis with deep convolutional neural networks](https://pdfs.semanticscholar.org/9320/a229b450bee8384f218681634e039acd9c2f.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and embeddings can be downloaded from [here](https://drive.google.com/open?id=0B8xjf4y9r8jCdVFjVTZqdzZTbVU)\n",
    "\n",
    "\n",
    "\n",
    "Come prima cosa prepariamo i dati per il training facendo del preprocessing. Questo [tokenizer](https://github.com/jaredks/tweetokenize) applica delle semplici trasformazioni al tweet: \n",
    "\n",
    "- lowercase\n",
    "- mappa i numeri in ad un token speciale NUMBER\n",
    "- mappa il nome utente ad un carattere speciale USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tkn = Tokenizer()\n",
    "\n",
    "def preprocess(tweet):\n",
    "    return tkn.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'USERNAME',\n",
       " u'gas',\n",
       " u'by',\n",
       " u'my',\n",
       " u'house',\n",
       " u'hit',\n",
       " u'NUMBER',\n",
       " u'!',\n",
       " u'!',\n",
       " u'!',\n",
       " u'!',\n",
       " u\"i'm\",\n",
       " u'going',\n",
       " u'to',\n",
       " u'chapel',\n",
       " u'hill',\n",
       " u'on',\n",
       " u'sat',\n",
       " u'.',\n",
       " u':)',\n",
       " u'#lol']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"@bestuser Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :) #lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto possiamo preprocessare il training set di Semeval 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'excuse',\n",
       " u'the',\n",
       " u'connectivity',\n",
       " u'of',\n",
       " u'this',\n",
       " u'live',\n",
       " u'stream',\n",
       " u',',\n",
       " u'from',\n",
       " u'baba',\n",
       " u'amr',\n",
       " u',',\n",
       " u'so',\n",
       " u'many',\n",
       " u'activists',\n",
       " u'using',\n",
       " u'only',\n",
       " u'one',\n",
       " u'sat',\n",
       " u'modem',\n",
       " u'.',\n",
       " u'LIVE',\n",
       " u'URL',\n",
       " u'#Homs']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(file_name, gold=None):\n",
    "    labels = {'negative':0, 'neutral':1, 'positive':2, 'unknwn':1}\n",
    "    X_, y_ = [], []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            label, _, text = line.strip().split('\\t') \n",
    "            y_.append(labels[label])\n",
    "            X_.append(preprocess(text))\n",
    "    if gold:\n",
    "        y_ = []\n",
    "        with open(gold) as f:\n",
    "            for line in f:\n",
    "                _, _, label = line.strip().split('\\t')\n",
    "                y_.append(labels[label])\n",
    "    return (X_, y_)\n",
    "            \n",
    "X_train, y_train = load_dataset(\"data/train'13.csv\")\n",
    "X_dev, y_dev = load_dataset(\"data/dev'13.csv\")\n",
    "X_train = X_train+X_dev\n",
    "y_train = y_train+y_dev\n",
    "X_dev, y_dev = load_dataset(\"data/test'13.csv\")\n",
    "X_test_1, _ = load_dataset(\"data/sms'13.csv\")\n",
    "X_test_2, _ = load_dataset(\"data/test'14.csv\")\n",
    "X_test_3, _ = load_dataset(\"data/test'15.csv\")\n",
    "X_dev[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per velocizzare il mapping assegnamo creiamo un dizionario dove ad ogni parola è assegnato un Id univoco. In questo dizionario aggiungiamo una token speciale per le parole sconosciute e uno per il PADDING (Spigherò dopo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33609"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "dictionary = {'PAD':0, 'UNK':1}\n",
    "\n",
    "toks = set(chain.from_iterable(X_train+X_dev+X_test_1+X_test_2+X_test_3))\n",
    "for i, tok in enumerate(toks):\n",
    "    dictionary[tok] = i+2\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora mappiamo le parole di training e dev set a questi indici nel dizionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20017,\n",
       " 10819,\n",
       " 13784,\n",
       " 15722,\n",
       " 26775,\n",
       " 11018,\n",
       " 32099,\n",
       " 29495,\n",
       " 14115,\n",
       " 9877,\n",
       " 3011,\n",
       " 29495,\n",
       " 17467,\n",
       " 17670,\n",
       " 1314,\n",
       " 18253,\n",
       " 19701,\n",
       " 733,\n",
       " 25967,\n",
       " 26452,\n",
       " 337,\n",
       " 265,\n",
       " 9510,\n",
       " 29007]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2id(sent):\n",
    "    return map(lambda x: dictionary.get(x, 1), sent)\n",
    "\n",
    "X_train = map(word2id, X_train)\n",
    "X_dev = map(word2id, X_dev)\n",
    "    \n",
    "X_dev[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generale le reti neurali accettano solo vettori (tensori) di dimensione prefissata quindi mapperemo tutte le frasi alla frase piu lunga del training set. E convertiamo le frasi in vettori numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0, 14158, 10819, 32671,\n",
       "       32837, 22163, 10819,  4200, 26828, 27247, 25524, 32837, 18384,\n",
       "       28365,  2483, 17877,  5846,  7431, 22140, 25735, 14638, 28244,\n",
       "       13358, 12938,   105, 16485,   337], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_len = max(len(x) for x in X_train)\n",
    "\n",
    "def _pad(s, maxlen):\n",
    "    pad_ = np.zeros(maxlen, dtype='int32')\n",
    "    trunc = np.asarray(s[-maxlen:], dtype='int32')\n",
    "    pad_[-len(trunc):] = trunc\n",
    "    return pad_\n",
    "\n",
    "X_train = np.array(map(lambda x: _pad(x, max_len), X_train))\n",
    "X_dev = np.array(map(lambda x: _pad(x, max_len), X_dev))\n",
    "X_train[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come ultimo passaggio mappiamo le labels delle tre classi in one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       ..., \n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labels(x):\n",
    "    out_ = np.zeros(3, dtype='int32')\n",
    "    out_[x] = 1\n",
    "    return out_\n",
    "\n",
    "y_train = np.array(map(labels, y_train))\n",
    "y_dev = np.array(map(labels, y_dev))\n",
    "y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "A questo punto definiamo la network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 68, 100)           3360900   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 68, 300)           150300    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 3,512,103\n",
      "Trainable params: 3,512,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Conv1D,\n",
    "                          GlobalMaxPooling1D,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), emb_dim, input_length=max_len)) #Embed\n",
    "model.add(Conv1D(filters=conv_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D()) #Encode\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 7s - loss: 1.0089 - acc: 0.5080 - val_loss: 0.9920 - val_acc: 0.5143\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.9924 - acc: 0.5050 - val_loss: 0.9819 - val_acc: 0.5143\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.9731 - acc: 0.5060 - val_loss: 0.9687 - val_acc: 0.5159\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.9556 - acc: 0.5350 - val_loss: 0.9664 - val_acc: 0.5177\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.9339 - acc: 0.5450 - val_loss: 0.9556 - val_acc: 0.5363\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.9126 - acc: 0.5680 - val_loss: 0.9495 - val_acc: 0.5350\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.8885 - acc: 0.5890 - val_loss: 0.9431 - val_acc: 0.5410\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.8604 - acc: 0.6120 - val_loss: 0.9443 - val_acc: 0.5366\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.8301 - acc: 0.6240 - val_loss: 0.9273 - val_acc: 0.5510\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.7971 - acc: 0.6570 - val_loss: 0.9307 - val_acc: 0.5489\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.7603 - acc: 0.6740 - val_loss: 0.9280 - val_acc: 0.5534\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.7220 - acc: 0.7010 - val_loss: 0.9058 - val_acc: 0.5625\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.6804 - acc: 0.7340 - val_loss: 0.9027 - val_acc: 0.5673\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.6394 - acc: 0.7700 - val_loss: 0.9071 - val_acc: 0.5759\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.5972 - acc: 0.7990 - val_loss: 0.9023 - val_acc: 0.5783\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.5554 - acc: 0.8270 - val_loss: 0.9002 - val_acc: 0.5770\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.5150 - acc: 0.8370 - val_loss: 0.9031 - val_acc: 0.5843\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.4746 - acc: 0.8700 - val_loss: 0.9126 - val_acc: 0.5785\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.4352 - acc: 0.8910 - val_loss: 0.9177 - val_acc: 0.5796\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.3977 - acc: 0.9090 - val_loss: 0.8988 - val_acc: 0.5877\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.3605 - acc: 0.9250 - val_loss: 0.9213 - val_acc: 0.5864\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.3275 - acc: 0.9400 - val_loss: 0.9063 - val_acc: 0.5903\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.2962 - acc: 0.9510 - val_loss: 0.9200 - val_acc: 0.5903\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.2645 - acc: 0.9630 - val_loss: 0.9237 - val_acc: 0.5903\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.2364 - acc: 0.9670 - val_loss: 0.9336 - val_acc: 0.5888\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.2098 - acc: 0.9740 - val_loss: 0.9498 - val_acc: 0.5872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11605d2d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train[:1000], y_train[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbonadiman/anaconda/envs/tensorflow/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semeval F1 score: 21.8996062992 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       394\n",
      "          1       0.83      0.38      0.52      1208\n",
      "          2       0.29      0.90      0.44       492\n",
      "\n",
      "avg / total       0.55      0.43      0.40      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 29.9707126586 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       601\n",
      "          1       0.58      0.81      0.67      1640\n",
      "          2       0.61      0.59      0.60      1572\n",
      "\n",
      "avg / total       0.50      0.59      0.54      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 29.7563504406 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       365\n",
      "          1       0.54      0.82      0.65       987\n",
      "          2       0.64      0.55      0.60      1038\n",
      "\n",
      "avg / total       0.50      0.58      0.53      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load_word2vec_format('data/embeddings.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_matrix(dictionary, model):\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(dictionary), 100))\n",
    "    for word in dictionary:\n",
    "        if word in model:\n",
    "            embedding_matrix[dictionary[word]] = model[word]\n",
    "    return embedding_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 68, 100)           3360900   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 68, 300)           150300    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 3,512,103\n",
      "Trainable params: 3,512,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Convolution1D,\n",
    "                          GlobalMaxPooling1D,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "embeddings = Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embeddings) #Embed\n",
    "model.add(Conv1D(filters=conv_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D()) #Encode\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 7s - loss: 1.0314 - acc: 0.4660 - val_loss: 0.9748 - val_acc: 0.5909\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.9358 - acc: 0.6420 - val_loss: 0.9227 - val_acc: 0.6181\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.8445 - acc: 0.6870 - val_loss: 0.8718 - val_acc: 0.6140\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.7590 - acc: 0.7200 - val_loss: 0.8382 - val_acc: 0.6192\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.6811 - acc: 0.7620 - val_loss: 0.8037 - val_acc: 0.6475\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 9s - loss: 0.6100 - acc: 0.8100 - val_loss: 0.7880 - val_acc: 0.6415\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.5457 - acc: 0.8430 - val_loss: 0.7659 - val_acc: 0.6709\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 8s - loss: 0.4869 - acc: 0.8760 - val_loss: 0.7512 - val_acc: 0.6740\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.4374 - acc: 0.8950 - val_loss: 0.7498 - val_acc: 0.6737\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 6s - loss: 0.3914 - acc: 0.9260 - val_loss: 0.7634 - val_acc: 0.6627\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.3487 - acc: 0.9390 - val_loss: 0.7472 - val_acc: 0.6696\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 7s - loss: 0.3112 - acc: 0.9480 - val_loss: 0.7675 - val_acc: 0.6633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12407fa10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train[:1000], y_train[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n",
      "Semeval F1 score: 52.3418466508 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.55      0.48       394\n",
      "          1       0.89      0.46      0.60      1208\n",
      "          2       0.43      0.84      0.57       492\n",
      "\n",
      "avg / total       0.69      0.56      0.57      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 55.8681577527 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.30      0.42       601\n",
      "          1       0.66      0.77      0.71      1640\n",
      "          2       0.68      0.72      0.70      1572\n",
      "\n",
      "avg / total       0.68      0.67      0.66      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 53.7115973045 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.32      0.40       365\n",
      "          1       0.62      0.79      0.70       987\n",
      "          2       0.71      0.64      0.67      1038\n",
      "\n",
      "avg / total       0.65      0.65      0.64      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 68, 100)           3360900   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 300)               360900    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 3,722,703\n",
      "Trainable params: 3,722,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          GRU,\n",
    "                          Bidirectional,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], trainable=True)) #Embed\n",
    "model.add(GRU(300, activation='tanh'))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 23s - loss: 1.0520 - acc: 0.4560 - val_loss: 1.0000 - val_acc: 0.4820\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.9841 - acc: 0.5090 - val_loss: 0.9582 - val_acc: 0.5308\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 27s - loss: 0.9425 - acc: 0.5560 - val_loss: 0.9175 - val_acc: 0.5628\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 25s - loss: 0.8830 - acc: 0.5890 - val_loss: 0.9120 - val_acc: 0.5641\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 25s - loss: 0.8399 - acc: 0.6130 - val_loss: 0.8662 - val_acc: 0.5969\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.8018 - acc: 0.6400 - val_loss: 0.8990 - val_acc: 0.5806\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.7661 - acc: 0.6650 - val_loss: 0.8521 - val_acc: 0.6069\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.7267 - acc: 0.6900 - val_loss: 0.9272 - val_acc: 0.5875\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.7037 - acc: 0.6850 - val_loss: 0.9468 - val_acc: 0.5764\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.6664 - acc: 0.7140 - val_loss: 0.8873 - val_acc: 0.6098\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.6583 - acc: 0.7180 - val_loss: 0.9072 - val_acc: 0.6058\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.6261 - acc: 0.7250 - val_loss: 0.9029 - val_acc: 0.6134\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.5934 - acc: 0.7600 - val_loss: 0.9149 - val_acc: 0.6082\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.5791 - acc: 0.7650 - val_loss: 0.9001 - val_acc: 0.6066\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.5466 - acc: 0.7830 - val_loss: 1.0417 - val_acc: 0.5932\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 22s - loss: 0.5224 - acc: 0.7930 - val_loss: 1.0599 - val_acc: 0.5974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1351e2810>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train[:1000], y_train[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n",
      "Semeval F1 score: 48.7468812026 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.57      0.44       394\n",
      "          1       0.81      0.47      0.59      1208\n",
      "          2       0.44      0.69      0.53       492\n",
      "\n",
      "avg / total       0.64      0.54      0.55      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 54.2490282103 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.45      0.47       601\n",
      "          1       0.60      0.72      0.66      1640\n",
      "          2       0.68      0.56      0.61      1572\n",
      "\n",
      "avg / total       0.62      0.61      0.61      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 49.5697541452 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.44      0.41       365\n",
      "          1       0.56      0.70      0.62       987\n",
      "          2       0.71      0.50      0.59      1038\n",
      "\n",
      "avg / total       0.60      0.57      0.57      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiniamo un convolutional model con un Recurrent model. Prima creaiamo le rappresentazioni dei 5-gram usando la convoluzione. Per velocizzare il training riduciamo la lunghezza dell'input usando Max pooling (4). Questo layer esegue l'operazione di max pooling non a livello di frase ma ogni 4 n-gram embeddings. In seguito usiamo gated recurrent unit per ottenere il vettore in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 68, 100)           3360900   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 68, 300)           150300    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 4,053,003\n",
      "Trainable params: 4,053,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dropout,\n",
    "                          Convolution1D,\n",
    "                          MaxPooling1D,\n",
    "                          GRU,\n",
    "                          Dense,\n",
    "                          Embedding)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.regularizers import l2\n",
    "\n",
    "emb_dim = 100\n",
    "conv_filters = 300\n",
    "\n",
    "embeddings = Embedding(len(dictionary), 100, input_length=max_len, weights=[emb_matrix(dictionary, w2v)], trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embeddings) #Embed\n",
    "model.add(Conv1D(filters=conv_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(4)) #Encode\n",
    "model.add(GRU(300, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) #Predict\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(lr=1.0, rho=0.90, epsilon=1e-8),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 3813 samples\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 14s - loss: 1.0500 - acc: 0.4350 - val_loss: 1.0004 - val_acc: 0.4390\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 14s - loss: 0.9761 - acc: 0.5330 - val_loss: 0.9275 - val_acc: 0.5570\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 14s - loss: 0.8744 - acc: 0.5990 - val_loss: 0.8807 - val_acc: 0.5772\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 12s - loss: 0.7700 - acc: 0.6860 - val_loss: 0.8477 - val_acc: 0.6150\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 13s - loss: 0.6780 - acc: 0.7230 - val_loss: 1.2088 - val_acc: 0.5059\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 15s - loss: 0.5722 - acc: 0.7600 - val_loss: 0.8502 - val_acc: 0.6126\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 12s - loss: 0.4604 - acc: 0.8270 - val_loss: 0.8811 - val_acc: 0.6260\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 14s - loss: 0.3837 - acc: 0.8540 - val_loss: 0.9430 - val_acc: 0.5901\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 15s - loss: 0.2987 - acc: 0.8910 - val_loss: 1.4869 - val_acc: 0.5397\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 13s - loss: 0.2325 - acc: 0.9340 - val_loss: 1.0561 - val_acc: 0.6119\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 12s - loss: 0.1471 - acc: 0.9650 - val_loss: 1.2650 - val_acc: 0.6019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124113690>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model_checkpoint = ModelCheckpoint('model.tra', save_best_only=True, mode='max', monitor='val_acc')\n",
    "\n",
    "model.fit(X_train[:1000], y_train[:1000],\n",
    "          batch_size=32,\n",
    "          epochs=1000,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sms'13.csv\n",
      "Semeval F1 score: 50.0319022878 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.50      0.44       394\n",
      "          1       0.79      0.57      0.66      1208\n",
      "          2       0.47      0.69      0.56       492\n",
      "\n",
      "avg / total       0.64      0.59      0.60      2094\n",
      "\n",
      "data/test'13.csv\n",
      "Semeval F1 score: 51.3382067927 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.28      0.38       601\n",
      "          1       0.61      0.74      0.67      1640\n",
      "          2       0.66      0.64      0.65      1572\n",
      "\n",
      "avg / total       0.63      0.63      0.61      3813\n",
      "\n",
      "data/test'15.csv\n",
      "Semeval F1 score: 49.80448975 %\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.29      0.37       365\n",
      "          1       0.58      0.76      0.66       987\n",
      "          2       0.68      0.58      0.63      1038\n",
      "\n",
      "avg / total       0.61      0.61      0.60      2390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model.tra')\n",
    "\n",
    "def evaluate(file_name, gold):\n",
    "    print(file_name)\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    X_, y_ = load_dataset(file_name, gold)\n",
    "    X_ = map(word2id, X_)\n",
    "    X_ = np.array(map(lambda x: _pad(x, max_len), X_))\n",
    "    pred = model.predict_classes(X_, verbose=0)\n",
    "    ev = precision_recall_fscore_support(y_, pred)\n",
    "    f1 = (ev[2][0]+ev[2][2])/2\n",
    "    print('Semeval F1 score: {} %'.format(f1*100))\n",
    "    print(classification_report(y_, pred))\n",
    "\n",
    "\n",
    "files = [\"data/sms'13.csv\",\n",
    "         \"data/test'13.csv\",\n",
    "         \"data/test'15.csv\"]\n",
    "\n",
    "gold = [None,\n",
    "        None,\n",
    "        \"data/SemEval2015-task10-test-B-gold.txt\"]\n",
    "\n",
    "\n",
    "for file_name, gold in zip(files, gold):\n",
    "    evaluate(file_name, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
