{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    data = [[]]\n",
    "    ners = [[]]\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                data.append([])\n",
    "                ners.append([])\n",
    "            else:\n",
    "                word, _, _, label = line.strip().split()\n",
    "                if word == '-DOCSTART-':\n",
    "                    data.pop()\n",
    "                    ners.pop()\n",
    "                    continue\n",
    "                else:\n",
    "                    data[-1].append(word)\n",
    "                    ners[-1].append(label)\n",
    "        #print(word, label)\n",
    "    data.pop()\n",
    "    ners.pop()\n",
    "    return data, ners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data, ner = read_data('data/eng.train')\n",
    "train = pd.DataFrame(data={'sentences':data, 'labels':ner})\n",
    "data, ner = read_data('data/eng.testa')\n",
    "dev = pd.DataFrame(data={'sentences':data, 'labels':ner})\n",
    "data, ner = read_data('data/eng.testb')\n",
    "test = pd.DataFrame(data={'sentences':data, 'labels':ner})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['low'] = train['sentences'].map(lambda x: [word.lower() for word in x])\n",
    "dev['low'] = dev['sentences'].map(lambda x: [word.lower() for word in x])\n",
    "test['low'] = test['sentences'].map(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23836"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load_word2vec_format('data/aquaint+wiki.txt.gz.ndim=50.bin', binary=True)\n",
    "dictionary = {'PAD':0, 'UNK':1}\n",
    "\n",
    "toks = (set(chain.from_iterable(train['low'])) | set(chain.from_iterable(test['low'])) | \\\n",
    "       set(chain.from_iterable(dev['low'])))\n",
    "\n",
    "i = 2\n",
    "for _, tok in enumerate(toks):\n",
    "    if tok in w2v:\n",
    "        dictionary[tok] = i\n",
    "        i+=1\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(sent):\n",
    "    return map(lambda x: dictionary.get(x, 1), sent)\n",
    "\n",
    "train['wids'] = train['low'].map(word2id)\n",
    "dev['wids'] = dev['low'].map(word2id)\n",
    "test['wids'] = test['low'].map(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def case(word):\n",
    "    if word.isupper():\n",
    "        return 2\n",
    "    elif word.istitle():\n",
    "        return 3\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return 4\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "train['cap'] = train['sentences'].map(lambda x: [case(word) for word in x])\n",
    "dev['cap'] = dev['sentences'].map(lambda x: [case(word) for word in x])\n",
    "test['cap'] = test['sentences'].map(lambda x: [case(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numb(word):\n",
    "    if all(char.isdigit() for char in word):\n",
    "        return 2\n",
    "    elif any(char.isdigit() for char in word):\n",
    "        return 3\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "train['num'] = train['sentences'].map(lambda x: [numb(word) for word in x])\n",
    "dev['num'] = dev['sentences'].map(lambda x: [numb(word) for word in x])\n",
    "test['num'] = test['sentences'].map(lambda x: [numb(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 1, 'B-ORG': 2, 'I-PER': 4, 'PAD': 0, 'O': 3, 'I-MISC': 5, 'B-MISC': 6, 'I-ORG': 7, 'B-LOC': 8}\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "classes = set(chain.from_iterable(pd.concat([train['labels'],dev['labels'],test['labels']])))\n",
    "labels_dict = {'PAD':0}\n",
    "for idx, lab in enumerate(classes, 1):\n",
    "    labels_dict[lab] = idx\n",
    "print(labels_dict)\n",
    "maxlen = max(len(sent) for sent in pd.concat([train['sentences'],dev['sentences'],test['sentences']]))\n",
    "print(maxlen)\n",
    "\n",
    "    \n",
    "train['lids'] = train['labels'].map(lambda sent: map(lambda w: labels_dict[w], sent))\n",
    "dev['lids'] = dev['labels'].map(lambda sent: map(lambda w: labels_dict[w], sent))\n",
    "test['lids'] = test['labels'].map(lambda sent: map(lambda w: labels_dict[w], sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train['y_g'] = train['lids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "train['X'] = train['wids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "train['X_num'] = train['num'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "train['X_cap'] = train['cap'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "\n",
    "dev['y_g'] = dev['lids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "dev['X'] = dev['wids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "dev['X_num'] = dev['num'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "dev['X_cap'] = dev['cap'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "\n",
    "test['y_g'] = test['lids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "test['X'] = test['wids'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "test['X_num'] = test['num'].apply(lambda s: pad_sequences([s], maxlen)[0])\n",
    "test['X_cap'] = test['cap'].apply(lambda s: pad_sequences([s], maxlen)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>sentences</th>\n",
       "      <th>low</th>\n",
       "      <th>wids</th>\n",
       "      <th>cap</th>\n",
       "      <th>num</th>\n",
       "      <th>lids</th>\n",
       "      <th>y_g</th>\n",
       "      <th>X</th>\n",
       "      <th>X_num</th>\n",
       "      <th>X_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[eu, rejects, german, call, to, boycott, briti...</td>\n",
       "      <td>[15655, 14924, 4010, 17820, 8821, 16288, 3898,...</td>\n",
       "      <td>[2, 1, 3, 1, 1, 1, 3, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[7, 3, 5, 3, 3, 3, 5, 3, 3]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I-PER, I-PER]</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[peter, blackburn]</td>\n",
       "      <td>[5650, 13774]</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I-LOC, O]</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[brussels, 1996-08-22]</td>\n",
       "      <td>[6081, 1]</td>\n",
       "      <td>[2, 1]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[the, european, commission, said, on, thursday...</td>\n",
       "      <td>[15201, 546, 14547, 18124, 22194, 10190, 15841...</td>\n",
       "      <td>[3, 3, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 7, 7, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...</td>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[germany, 's, representative, to, the, europea...</td>\n",
       "      <td>[3242, 712, 1931, 8821, 15201, 546, 494, 712, ...</td>\n",
       "      <td>[3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 3, 3, 3, 3, 7, 7, 3, 3, 3, 4, 4, 3, 3, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              labels  \\\n",
       "0          [I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]   \n",
       "1                                     [I-PER, I-PER]   \n",
       "2                                         [I-LOC, O]   \n",
       "3  [O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...   \n",
       "4  [I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1                                 [Peter, Blackburn]   \n",
       "2                             [BRUSSELS, 1996-08-22]   \n",
       "3  [The, European, Commission, said, on, Thursday...   \n",
       "4  [Germany, 's, representative, to, the, Europea...   \n",
       "\n",
       "                                                 low  \\\n",
       "0  [eu, rejects, german, call, to, boycott, briti...   \n",
       "1                                 [peter, blackburn]   \n",
       "2                             [brussels, 1996-08-22]   \n",
       "3  [the, european, commission, said, on, thursday...   \n",
       "4  [germany, 's, representative, to, the, europea...   \n",
       "\n",
       "                                                wids  \\\n",
       "0  [15655, 14924, 4010, 17820, 8821, 16288, 3898,...   \n",
       "1                                      [5650, 13774]   \n",
       "2                                          [6081, 1]   \n",
       "3  [15201, 546, 14547, 18124, 22194, 10190, 15841...   \n",
       "4  [3242, 712, 1931, 8821, 15201, 546, 494, 712, ...   \n",
       "\n",
       "                                                 cap  \\\n",
       "0                        [2, 1, 3, 1, 1, 1, 3, 1, 1]   \n",
       "1                                             [3, 3]   \n",
       "2                                             [2, 1]   \n",
       "3  [3, 3, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, ...   \n",
       "4  [3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, ...   \n",
       "\n",
       "                                                 num  \\\n",
       "0                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "1                                             [1, 1]   \n",
       "2                                             [1, 3]   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                lids  \\\n",
       "0                        [7, 3, 5, 3, 3, 3, 5, 3, 3]   \n",
       "1                                             [4, 4]   \n",
       "2                                             [1, 3]   \n",
       "3  [3, 7, 7, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, ...   \n",
       "4  [1, 3, 3, 3, 3, 7, 7, 3, 3, 3, 4, 4, 3, 3, 3, ...   \n",
       "\n",
       "                                                 y_g  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               X_num  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               X_cap  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "def labels_to_prob(labs):\n",
    "    return np.vstack([to_categorical(lab, len(classes)+1) for lab in labs])\n",
    "    \n",
    "train['y'] = train['y_g'].map(labels_to_prob)\n",
    "dev['y'] = dev['y_g'].map(labels_to_prob)\n",
    "test['y'] = test['y_g'].map(labels_to_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evalu(y_all, y_gold, tokens):\n",
    "\n",
    "    tokens = dict((i, lab) for lab, i in tokens.items())\n",
    "    tagged_y = []\n",
    "    tagged_gold = []\n",
    "    with open(\"out.txt\", 'w') as f:\n",
    "        for seq, gold in zip(y_all, y_gold):\n",
    "            seq =  seq[-len(gold):]\n",
    "            tmp_y = []\n",
    "            tmp_g = []\n",
    "            for y_word, g_word in zip(seq, gold):\n",
    "                tmp_y.append(tokens[y_word] if y_word !=0 else u'0')\n",
    "                tmp_g.append(tokens[g_word])\n",
    "            tagged_y.append(tmp_y)\n",
    "            tagged_gold.append(tmp_g)\n",
    "        for y_seq, g_seq in zip(tagged_y, tagged_gold):\n",
    "            for y_word, g_word in zip(y_seq,g_seq):\n",
    "                f.write(\" \".join([\"WORD\",\"POS\", g_word, y_word])+'\\n')\n",
    "            f.write('\\n')\n",
    "    \n",
    "    \n",
    "evalu([[0, 2, 3]], [[0,0,0]], labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 3 tokens with 0 phrases; found: 1 phrases; correct: 0.\r\n",
      "accuracy:   0.00%; precision:   0.00%; recall:   0.00%; FB1:   0.00\r\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\r\n"
     ]
    }
   ],
   "source": [
    "!env LANG=C perl conlleval.pl < out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emb_matrix(dictionary, model):\n",
    "    embedding_matrix = np.zeros((len(dictionary), 50))\n",
    "    for word in dictionary:\n",
    "        if word in model:\n",
    "            embedding_matrix[dictionary[word]] = model[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 124, 50)           1191800   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 124, 100)          5100      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 124, 9)            909       \n",
      "=================================================================\n",
      "Total params: 1,197,809\n",
      "Trainable params: 1,197,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (Input,\n",
    "                          Embedding,\n",
    "                          Convolution1D,\n",
    "                          TimeDistributed,\n",
    "                          Dense)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), 50 ,input_length=maxlen, weights=[emb_matrix(dictionary, w2v)], trainable=True, mask_zero=True))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dense(len(classes)+1, activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3251 samples\n",
      "Epoch 1/5\n",
      "14041/14041 [==============================] - 12s - loss: 0.2489 - acc: 0.9296 - val_loss: 0.1650 - val_acc: 0.9502\n",
      "Epoch 2/5\n",
      "14041/14041 [==============================] - 10s - loss: 0.1167 - acc: 0.9617 - val_loss: 0.1641 - val_acc: 0.9487\n",
      "Epoch 3/5\n",
      "14041/14041 [==============================] - 10s - loss: 0.1065 - acc: 0.9636 - val_loss: 0.1659 - val_acc: 0.9508\n",
      "Epoch 4/5\n",
      "14041/14041 [==============================] - 10s - loss: 0.1024 - acc: 0.9641 - val_loss: 0.1698 - val_acc: 0.9495\n",
      "Epoch 5/5\n",
      "14041/14041 [==============================] - 12s - loss: 0.1010 - acc: 0.9644 - val_loss: 0.1709 - val_acc: 0.9503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1976cff50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.vstack(train['X'].tolist()),\n",
    "          np.array(train['y'].tolist()),\n",
    "          batch_size=100,\n",
    "          epochs=5,\n",
    "          validation_data=(np.vstack(dev['X'].tolist()), np.array(dev['y'].tolist())),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13696/14041 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(np.vstack(train['X'].tolist()))\n",
    "gold = np.array(train['y_g'].tolist())\n",
    "evalu(preds, gold, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1741084 tokens with 23499 phrases; found: 24312 phrases; correct: 19168.\r\n",
      "accuracy:  11.30%; precision:  78.84%; recall:  81.57%; FB1:  80.18\r\n",
      "              LOC: precision:  79.46%; recall:  87.59%; FB1:  83.33  7871\r\n",
      "             MISC: precision:  77.23%; recall:  72.51%; FB1:  74.80  3228\r\n",
      "              ORG: precision:  68.39%; recall:  69.45%; FB1:  68.92  6419\r\n",
      "              PER: precision:  88.77%; recall:  91.38%; FB1:  90.06  6794\r\n"
     ]
    }
   ],
   "source": [
    "!env LANG=C perl conlleval.pl < out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880/3251 [=========================>....] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(np.vstack(dev['X'].tolist()))\n",
    "gold = np.array(dev['y_g'].tolist())\n",
    "evalu(preds, gold, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 403124 tokens with 5942 phrases; found: 6107 phrases; correct: 4468.\r\n",
      "accuracy:  12.14%; precision:  73.16%; recall:  75.19%; FB1:  74.16\r\n",
      "              LOC: precision:  78.16%; recall:  82.58%; FB1:  80.31  1941\r\n",
      "             MISC: precision:  69.20%; recall:  62.15%; FB1:  65.49  828\r\n",
      "              ORG: precision:  57.40%; recall:  60.70%; FB1:  59.01  1418\r\n",
      "              PER: precision:  81.46%; recall:  84.91%; FB1:  83.15  1920\r\n"
     ]
    }
   ],
   "source": [
    "!env LANG=C perl conlleval.pl < out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 124)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 124)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 124, 50)       1191800     input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 124, 5)        25          input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 124, 55)       0           embedding_9[0][0]                \n",
      "                                                                   embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)        (None, 124, 9)        6509        concatenate_3[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 1,198,334\n",
      "Trainable params: 1,198,334\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (Input,\n",
    "                          Embedding,\n",
    "                          Convolution1D,\n",
    "                          TimeDistributed,\n",
    "                          Dense,\n",
    "                          concatenate)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "word = Input(shape=(maxlen,))\n",
    "cap = Input(shape=(maxlen,))\n",
    "\n",
    "cap_emb = Embedding(5, 5,input_length=maxlen)(cap)\n",
    "word_emb = Embedding(len(dictionary), 50 , weights=[emb_matrix(dictionary, w2v)],input_length=maxlen, trainable=True)(word)\n",
    "\n",
    "emb = concatenate([word_emb, cap_emb])\n",
    "\n",
    "model_s = Sequential()\n",
    "model_s.add(TimeDistributed(Dense(100, activation='relu'), input_shape=(maxlen, 55)))\n",
    "model_s.add(TimeDistributed(Dense(len(classes)+1, activation='softmax')))\n",
    "\n",
    "out = model_s(emb)\n",
    "\n",
    "model = Model(inputs=[word, cap], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3251 samples\n",
      "Epoch 1/10\n",
      "14041/14041 [==============================] - 14s - loss: 0.1135 - acc: 0.9861 - val_loss: 0.0143 - val_acc: 0.9958\n",
      "Epoch 2/10\n",
      "14041/14041 [==============================] - 13s - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0144 - val_acc: 0.9959\n",
      "Epoch 3/10\n",
      "14041/14041 [==============================] - 12s - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0143 - val_acc: 0.9959\n",
      "Epoch 4/10\n",
      "14041/14041 [==============================] - 12s - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0144 - val_acc: 0.9960\n",
      "Epoch 5/10\n",
      "14041/14041 [==============================] - 10s - loss: 0.0073 - acc: 0.9974 - val_loss: 0.0149 - val_acc: 0.9960\n",
      "Epoch 6/10\n",
      "14041/14041 [==============================] - 11s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0150 - val_acc: 0.9960\n",
      "Epoch 7/10\n",
      "14041/14041 [==============================] - 11s - loss: 0.0070 - acc: 0.9975 - val_loss: 0.0150 - val_acc: 0.9959\n",
      "Epoch 8/10\n",
      "14041/14041 [==============================] - 10s - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0152 - val_acc: 0.9960\n",
      "Epoch 9/10\n",
      "14041/14041 [==============================] - 10s - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0154 - val_acc: 0.9960\n",
      "Epoch 10/10\n",
      "14041/14041 [==============================] - 13s - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0155 - val_acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b408be90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([np.vstack(train['X'].tolist()),np.vstack(train['X_cap'].tolist())],\n",
    "          np.array(train['y'].tolist()),\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          validation_data=([np.vstack(dev['X'].tolist()),np.vstack(dev['X_cap'].tolist())], np.array(dev['y'].tolist())),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = [np.argmax(x, axis=1) for x in model.predict([np.vstack(dev['X'].tolist()), np.vstack(dev['X_cap'].tolist())])]\n",
    "gold = np.array(dev['y_g'].tolist())\n",
    "evalu(preds, gold, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 403124 tokens with 5942 phrases; found: 6568 phrases; correct: 4801.\r\n",
      "accuracy:  12.33%; precision:  73.10%; recall:  80.80%; FB1:  76.75\r\n",
      "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\r\n",
      "              LOC: precision:  80.68%; recall:  83.89%; FB1:  82.25  1910\r\n",
      "             MISC: precision:  68.29%; recall:  73.10%; FB1:  70.61  987\r\n",
      "              ORG: precision:  54.74%; recall:  68.46%; FB1:  60.83  1677\r\n",
      "              PER: precision:  83.69%; recall:  90.55%; FB1:  86.99  1993\r\n"
     ]
    }
   ],
   "source": [
    "!env LANG=C perl conlleval.pl < out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 124, 50)           1191800   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 124, 100)          60400     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 124, 9)            909       \n",
      "=================================================================\n",
      "Total params: 1,253,109\n",
      "Trainable params: 1,253,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (Input,\n",
    "                          Embedding,\n",
    "                          Convolution1D,\n",
    "                          TimeDistributed,\n",
    "                          Dense)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(dictionary), 50 ,input_length=maxlen, weights=[emb_matrix(dictionary, w2v)], trainable=True, mask_zero=True))\n",
    "model.add(LSTM(100, activation='tanh', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(classes)+1, activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14041 samples, validate on 3251 samples\n",
      "Epoch 1/2\n",
      "14041/14041 [==============================] - 148s - loss: 0.2425 - acc: 0.9306 - val_loss: 0.1305 - val_acc: 0.9625\n",
      "Epoch 2/2\n",
      "14041/14041 [==============================] - 142s - loss: 0.0796 - acc: 0.9744 - val_loss: 0.1184 - val_acc: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d1c86b50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.vstack(train['X'].tolist()),\n",
    "          np.array(train['y'].tolist()),\n",
    "          batch_size=100,\n",
    "          epochs=2,\n",
    "          validation_data=(np.vstack(dev['X'].tolist()), np.array(dev['y'].tolist())),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3251/3251 [==============================] - 11s    \n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_classes(np.vstack(dev['X'].tolist()))\n",
    "gold = np.array(dev['y_g'].tolist())\n",
    "evalu(preds, gold, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 403124 tokens with 5942 phrases; found: 6039 phrases; correct: 4797.\r\n",
      "accuracy:  12.28%; precision:  79.43%; recall:  80.73%; FB1:  80.08\r\n",
      "              LOC: precision:  87.03%; recall:  84.76%; FB1:  85.88  1789\r\n",
      "             MISC: precision:  69.13%; recall:  71.15%; FB1:  70.12  949\r\n",
      "              ORG: precision:  63.10%; recall:  71.29%; FB1:  66.95  1515\r\n",
      "              PER: precision:  91.15%; recall:  88.38%; FB1:  89.75  1786\r\n"
     ]
    }
   ],
   "source": [
    "!env LANG=C perl conlleval.pl < out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CRF output is not supported in Keras yet. However you can find an implementation here\n",
    "\n",
    "\n",
    "https://github.com/phipleg/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully working implementation of state of the art networks for NER can be found there:\n",
    "\n",
    "https://github.com/glample/tagger\n",
    "\n",
    "That implements the state of art architecture https://arxiv.org/abs/1603.01360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
